{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictiveAnalytics_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Collection and Retrieval Function**"
      ],
      "metadata": {
        "id": "lmX7jdZx6mfw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZTI3n63XB2-"
      },
      "source": [
        "# Data Retrieval Function\n",
        "\n",
        "def get_data(search_term):\n",
        "  import pandas as pd\n",
        "  import requests\n",
        "  import json\n",
        "\n",
        "  # Insert your own bearer token after setting up your Twitter Developer account\n",
        "  bearer_token = ''\n",
        "  headers = {'Authorization':('Bearer '+ bearer_token)}\n",
        "\n",
        "  n = 550                          \n",
        "  max_results = 100                 \n",
        "  total_retrieved = 0            \n",
        "  next_token = \"\"                   \n",
        "  #search_term = \"self driving\"            \n",
        "  #since_id = \"100000\"  \n",
        "\n",
        "  # Create the empty DataFrame with the columns you want\n",
        "  df = pd.DataFrame(columns=['id', 'retweets', 'likes', 'url', 'text', 'lang', 'source', 'sensitive'])\n",
        "  df.set_index('id', inplace=True)\n",
        "\n",
        "  # stop when we have n results\n",
        "  while total_retrieved < n:\n",
        "\n",
        "    # the first time through the loop, we do not need the next_token parameter\n",
        "    if next_token == \"\":\n",
        "      url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\n",
        "    else:\n",
        "      url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\n",
        "\n",
        "    # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
        "    url += f'&tweet.fields=attachments,public_metrics,text,lang,source,possibly_sensitive'\n",
        "    url += f'&expansions=attachments.media_keys'\n",
        "    url += f'&media.fields=media_key,type,url'\n",
        "\n",
        "    # make the request to the Twitter API Recent Search endpoint\n",
        "    response = requests.request(\"GET\", url, headers=headers)\n",
        "    try:  # Just in case we get an error\n",
        "      json_data = json.loads(response.text)\n",
        "    except:\n",
        "      print(response.text)\n",
        "    \n",
        "\n",
        "    for tweet in json_data[\"data\"]:\n",
        "      media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
        "\n",
        "      # Store the data into variables\n",
        "      tweet_id = tweet['id']\n",
        "      retweet_count = tweet['public_metrics']['retweet_count']\n",
        "      like_count = tweet['public_metrics']['like_count']\n",
        "      image_url = \"\"\n",
        "      text = tweet['text']\n",
        "      language = tweet['lang']\n",
        "      source = tweet['source']\n",
        "      sensitive = tweet['possibly_sensitive']\n",
        "\n",
        "      # Find out if there is media\n",
        "      if 'attachments' in tweet:\n",
        "        if 'media_keys' in tweet['attachments']:\n",
        "          media_key = tweet['attachments']['media_keys'][0]\n",
        "\n",
        "      # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
        "      if media_key != \"\":\n",
        "        for media in json_data['includes']['media']:\n",
        "          if media['media_key'] == media_key: # Only if the media_key matches the one we stored\n",
        "            if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
        "              image_url = media['url']        # Store the url in a variable\n",
        "              \n",
        "              # Only iterate if a photo is found\n",
        "              total_retrieved += 1\n",
        "              \n",
        "              # Only add the record in the DataFrame if a photo is found\n",
        "              df.loc[tweet_id] = [retweet_count, like_count, image_url, text, language, source, sensitive]\n",
        "              break\n",
        "\n",
        "    # keep track of where to start next time, but quit if there are no more results\n",
        "    try:\n",
        "      next_token = json_data['meta']['next_token']\n",
        "    except:\n",
        "      break  \n",
        "\n",
        "  return df\n",
        "\n",
        "# df = get_data('self driving')\n",
        "# print(f'Number of records:\\t{len(df)}')\n",
        "# df.to_csv('twitter.csv')\n",
        "# df.head()\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Cleaning Functions**"
      ],
      "metadata": {
        "id": "5bTtedOtvIit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Functions\n",
        "\n",
        "def bin_groups(df, percent=.05):\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col])  and col not in [\"url\", \"text\", \"source\"]:\n",
        "      for group, count in df[col].value_counts().iteritems():\n",
        "        if count / len(df) < percent:\n",
        "          df.loc[df[col] == group, col] = 'Other'\n",
        "  return df\n",
        "\n",
        "# Remove columns with more than 50% missing data\n",
        "def drop_columns_missing_data(df, cutoff=.5):\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if df[col].isna().sum() / len(df) > cutoff:\n",
        "      df.drop(columns=[col], inplace=True)\n",
        "  return df\n",
        "\n",
        "def impute_mean(df):\n",
        "  from sklearn.impute import SimpleImputer\n",
        "  import pandas as pd, numpy as np\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "  imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
        "  return df\n",
        "\n",
        "def impute_KNN(df):\n",
        "  from sklearn.impute import KNNImputer\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "  df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns = df.columns)\n",
        "  imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
        "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
        "  return df\n",
        "          \n",
        "def impute_reg(df):\n",
        "  from sklearn.experimental import enable_iterative_imputer\n",
        "  from sklearn.impute import IterativeImputer\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "  imp = IterativeImputer(max_iter=10, random_state=12345)\n",
        "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
        "  return df\n",
        "\n",
        "# ------------------------Additional Cleaning---------------------------------------------------------\n",
        "\n",
        "#remove retweets, get only original tweets\n",
        "def remove_retweets(df):\n",
        "  #print(f'Total tweets: {len(df)}')\n",
        "  df = df[~df['text'].str.contains(\"RT @\")]\n",
        "  #print(f'Original tweets: {len(df_originals)}')\n",
        "  return df\n",
        "\n",
        "\n",
        "def bin_source_label_groups(df): \n",
        "  #df_new = df_originals.copy()\n",
        "\n",
        "  pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "  for row in df.itertuples():\n",
        "    if row[6] in ['Twitter Web App', 'Twitter'] or \"Twitter for \" in row[6]:\n",
        "      df.at[row[0], 'source'] = 'Direct to Twitter'\n",
        "    elif \"bot\" in row[6].lower():\n",
        "      df.at[row[0], 'source'] = 'Bot'\n",
        "  #print(df.source.value_counts() / len(df))\n",
        "  return df\n",
        "\n",
        "\n",
        "def dummy_codes(df):\n",
        "  df_dummies = df.copy()\n",
        "\n",
        "  df[\"retweets\"] = pd.to_numeric(df[\"retweets\"])\n",
        "  df[\"likes\"] = pd.to_numeric(df[\"likes\"])\n",
        "\n",
        "  for col in df_dummies:\n",
        "    if not pd.api.types.is_numeric_dtype(df_dummies[col]) and col not in [\"url\", \"text\", 'retweets', 'likes']:\n",
        "      df_dummies = pd.get_dummies(df_dummies, columns=[col], drop_first=True)\n",
        "\n",
        "  # print(df_dummies.shape)\n",
        "  # df_dummies.head()\n",
        "  return df_dummies\n",
        "\n",
        "\n",
        "def get_sentiment(df):\n",
        "  import nltk\n",
        "  from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "  nltk.download('vader_lexicon')\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "  df_tweets = df.copy()\n",
        "  df_tweets['sentiment_overall'] = 0.0\n",
        "  df_tweets['sentiment_neg'] = 0.0\n",
        "  df_tweets['sentiment_neu'] = 0.0\n",
        "  df_tweets['sentiment_pos'] = 0.0\n",
        "\n",
        "  for row in df_tweets.itertuples():\n",
        "    sentiment = sia.polarity_scores(row[4])\n",
        "    df_tweets.loc[row[0], 'sentiment_overall'] = sentiment['compound']\n",
        "    df_tweets.loc[row[0], 'sentiment_neg'] = sentiment['neg']\n",
        "    df_tweets.loc[row[0], 'sentiment_neu'] = sentiment['neu']\n",
        "    df_tweets.loc[row[0], 'sentiment_pos'] = sentiment['pos']\n",
        "\n",
        "  #df_tweets.head()\n",
        "  return df_tweets\n",
        "\n",
        "\n",
        "def image_cleaning(df): \n",
        "  # !pip install pytesseract\n",
        "  # !sudo apt install tesseract-ocr\n",
        "\n",
        "  from skimage.io import imread\n",
        "  from PIL import Image\n",
        "  import requests\n",
        "  from io import BytesIO\n",
        "  import cv2\n",
        "  from pathlib import Path\n",
        "  import numpy as np\n",
        "  import pytesseract\n",
        "  from pathlib import Path\n",
        "  import re\n",
        "\n",
        "  # Needed if using Colab\n",
        "  from google.colab.patches import cv2_imshow\n",
        "\n",
        "  df_eng = df.copy()\n",
        "  df['num_faces'] = 0\n",
        "  df['image_text'] = \"\"\n",
        "\n",
        "  face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Colab Notebooks/data/haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "  for row in df_eng.itertuples():\n",
        "    response = requests.get(row[3])\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img = img.save(row[3].partition(\"media/\")[2])\n",
        "    path = \"/content/\" + row[3].partition(\"media\")[2]\n",
        "    image = cv2.imread(path)\n",
        "    grayimg = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(image=grayimg, scaleFactor=1.3, minNeighbors=5)\n",
        "    #eyes = eye_cascade.detectMultiScale(image=gray, scaleFactor=1.05, minNeighbors=3)\n",
        "    df_eng.loc[row[0], 'num_faces'] = len(faces)\n",
        "    text = pytesseract.image_to_string(path)\n",
        "    df_eng.loc[row[0], 'image_text'] = re.sub('\\s+',' ', text)\n",
        "\n",
        "  df_eng.head()\n",
        "  return df"
      ],
      "metadata": {
        "id": "V6zNRovriMCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.isna().sum() / len(df)"
      ],
      "metadata": {
        "id": "aGJfjsbQbWb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modeling Functions - Imports / Initialization**"
      ],
      "metadata": {
        "id": "CnkDRRlEgdee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeling Functions \n",
        "\n",
        "def fs_variance(df, label=\"\", p=0.8):\n",
        "  from sklearn.feature_selection import VarianceThreshold\n",
        "  import pandas as pd\n",
        "  \n",
        "  if label != \"\":\n",
        "    X = df.drop(columns=[label])\n",
        "      \n",
        "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
        "  sel.fit_transform(X)\n",
        "  \n",
        "  # Add the label back in after removing poor features\n",
        "  return df[sel.get_feature_names_out()].join(df[label])\n",
        "\n",
        "def fit_mlr(df, test_size=.2, random_state=12345, label=''):\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import pandas as pd\n",
        "\n",
        "  numerical_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "  X = numerical_df.drop(label,axis=1)\n",
        "  y = numerical_df[label]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "  model = LinearRegression().fit(X_train, y_train)\n",
        "  print(f'R-squared (mlr): \\t{model.score(X_test, y_test)}')\n",
        "  return model\n",
        "\n",
        "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
        "  import pandas as pd\n",
        "  from numpy import mean, std\n",
        "\n",
        "  #select only numberic columns\n",
        "  df = df.select_dtypes(include=np.number)\n",
        "\n",
        "  X = df.drop(label,axis=1)\n",
        "  y = df[label]\n",
        "  if repeat:\n",
        "    cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
        "  else:\n",
        "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
        "  scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
        "  print(f'Average R-squared:\\t{mean(scores)}')\n",
        "  return LinearRegression().fit(X, y)\n",
        "\n",
        "\n",
        "def fi_selectkbest(df, label, k=10):\n",
        "  from sklearn.feature_selection import SelectKBest, r_regression\n",
        "  y = df[label]\n",
        "  X = df.drop(columns=[label])\n",
        "  sel = SelectKBest(r_regression, k=k)\n",
        "  sel.fit_transform(X, y)\n",
        "  return df[sel.get_feature_names_out()].join(df[label])\n",
        "\n",
        "def fi_select_linear(df, label):\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  from sklearn.feature_selection import SelectFromModel\n",
        "  y = df[label]\n",
        "  X = df.drop(columns=[label])\n",
        "\n",
        "  model = LinearRegression().fit(X, y)\n",
        "  sel = SelectFromModel(model, prefit=True)\n",
        "  sel.transform(X)\n",
        "\n",
        "  columns = list(X.columns[sel.get_support()])\n",
        "  columns.append(label)\n",
        "\n",
        "  return df[columns]\n",
        "\n",
        "def fi_select_trees(df, label):\n",
        "  from sklearn.ensemble import ExtraTreesClassifier\n",
        "  from sklearn.feature_selection import SelectFromModel\n",
        "  y = df[label]\n",
        "  X = df.drop(columns=[label])\n",
        "\n",
        "  model = ExtraTreesClassifier().fit(X, y)\n",
        "  sel = SelectFromModel(model, prefit=True)\n",
        "  sel.transform(X)\n",
        "\n",
        "  columns = list(X.columns[sel.get_support()])\n",
        "  columns.append(label)\n",
        "\n",
        "  return df[columns]\n"
      ],
      "metadata": {
        "id": "eopp2Cc9ipIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clustering Model**"
      ],
      "metadata": {
        "id": "KXwQIbOVFeZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering_model(df, label):\n",
        "  !pip install gower\n",
        "  from sklearn.cluster import AgglomerativeClustering\n",
        "  import  gower\n",
        "\n",
        "  X = df.drop(columns=[label])\n",
        "  y = df[label]\n",
        "\n",
        "  distance_matrix = gower.gower_matrix(X)\n",
        "  pd.DataFrame(distance_matrix).head()\n",
        "\n",
        "  agg = AgglomerativeClustering(affinity=\"precomputed\", linkage=\"average\").fit(distance_matrix)\n",
        "  df['agg_cluster'] = agg.labels_\n",
        "\n",
        "  print(df.agg_cluster.value_counts(), '\\n\\n')\n",
        "  return df"
      ],
      "metadata": {
        "id": "p-lDr4vMFdRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output Functions**"
      ],
      "metadata": {
        "id": "7EOFgaKOpKGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output\n",
        "def dump_pickle(model, file_name):\n",
        "  import pickle\n",
        "  pickle.dump(model, open(file_name, \"wb\"))\n",
        "\n",
        "def load_pickle(file_name):\n",
        "  import pickle\n",
        "  model = pickle.load(open(file_name, \"rb\"))\n",
        "  return model"
      ],
      "metadata": {
        "id": "9u3isBXYyXEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algorithm Selection Functions**"
      ],
      "metadata": {
        "id": "YU_350E38oqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Model Algorithm Selection (5 algorithms for regression)\n",
        "\n",
        "def fit_crossvalidate_reg(df, label, k=10, n=5, repeat=True):\n",
        "  import sklearn.linear_model as lm, sklearn.ensemble as se\n",
        "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
        "  import pandas as pd\n",
        "  from numpy import mean, std\n",
        "  from xgboost import XGBRegressor\n",
        "\n",
        "  X = df.drop(columns=[label])\n",
        "  y = df[label]\n",
        "\n",
        "  if repeat:\n",
        "    cv = RepeatedKFold(n_splits=k, n_repeats=n, random_state=12345)\n",
        "  else:\n",
        "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
        "\n",
        "  fit = {}\n",
        "  model = {}\n",
        "\n",
        "  model_lr = lm.LinearRegression()\n",
        "  model_ridge = lm.Ridge()\n",
        "  #model_sgd = lm.SGDRegressor(max_iter=1000, tol=1e-3)\n",
        "  model_lasso = lm.Lasso(alpha=0.1)\n",
        "  model_ada = se.AdaBoostRegressor(random_state=12345, n_estimators=100)\n",
        "  model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
        "\n",
        "  fit['MLR'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['Ridge'] = mean(cross_val_score(model_ridge, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  #fit['SGD'] = mean(cross_val_score(model_sgd, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['Lasso'] = mean(cross_val_score(model_lasso, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "\n",
        "  model['MLR'] = model_lr\n",
        "  model['Ridge'] = model_ridge\n",
        "  #model['SGD'] = model_sgd\n",
        "  model['Lasso'] = model_lasso\n",
        "  model['AdaBoost'] = model_ada\n",
        "  model['XGBoost'] = model_xgb\n",
        "\n",
        "  df_fit = pd.DataFrame({'R-squared':fit})\n",
        "  df_fit = df_fit.sort_values(by=['R-squared'], ascending=False)\n",
        "\n",
        "  print(df_fit)\n",
        "\n",
        "  best_model = df_fit.index[0]\n",
        "  return model[best_model].fit(X, y)\n",
        "\n"
      ],
      "metadata": {
        "id": "gweX_DrHPoOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Model Algorithm Selection (5 algorithms for classification)\n",
        "\n",
        "def fit_crossvalidate_clf(df, label, k=10, n=5, repeat=True):\n",
        "  import sklearn.linear_model as lm, sklearn.ensemble as se\n",
        "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
        "  import pandas as pd\n",
        "  from numpy import mean, std\n",
        "  from xgboost import XGBClassifier\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.naive_bayes import CategoricalNB\n",
        "\n",
        "  X = df.drop(columns=[label])\n",
        "  y = df[label]\n",
        "\n",
        "  if repeat:\n",
        "    cv = RepeatedKFold(n_splits=k, n_repeats=n, random_state=12345)\n",
        "  else:\n",
        "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
        "\n",
        "  fit = {}\n",
        "  model = {}\n",
        "\n",
        "  model_log = lm.LogisticRegression(max_iter=100)\n",
        "  model_knn = KNeighborsClassifier(n_neighbors=3)\n",
        "  #model_nb = CategoricalNB()\n",
        "  model_ada = se.AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
        "  model_ext = se.ExtraTreesClassifier(n_estimators=100, random_state=12345)\n",
        "  model_xgb = XGBClassifier()\n",
        "\n",
        "  fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  #fit['NaiveBayes'] = mean(cross_val_score(model_nb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['ExtraTrees'] = mean(cross_val_score(model_ext, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "\n",
        "  model['Logistic'] = model_log\n",
        "  model['KNN'] = model_knn\n",
        "  #model['NaiveBayes'] = model_nb\n",
        "  model['AdaBoost'] = model_ada\n",
        "  model['ExtraTrees'] = model_ext\n",
        "  model['XGBoost'] = model_xgb\n",
        "\n",
        "  df_fit = pd.DataFrame({'Accuracy':fit})\n",
        "  df_fit = df_fit.sort_values(by=['Accuracy'], ascending=False)\n",
        "\n",
        "  print(df_fit)\n",
        "\n",
        "  best_model = df_fit.index[0]\n",
        "  return model[best_model].fit(X, y)"
      ],
      "metadata": {
        "id": "f9kOjoNYgoH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline Function Calls**"
      ],
      "metadata": {
        "id": "067ux05G8cGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression Model Pipeline\n",
        "# LABEL = likes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data retrival pipeline\n",
        "data = get_data('self driving')\n",
        "df = data\n",
        "\n",
        "# Data cleaning pipeline\n",
        "df = bin_source_label_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "# df = impute_KNN(df)    #not currently applicable since we don't have missing data\n",
        "df = remove_retweets(df)\n",
        "df = dummy_codes(df)\n",
        "df = get_sentiment(df)\n",
        "df[['retweets', 'likes']] = df[['retweets', 'likes']].apply(pd.to_numeric) # converting \"retweets\" and \"likes\" datatype from an object to numeric\n",
        "#df = image_cleaning(df)\n",
        "numerical_df = df.select_dtypes(include=np.number) # including only numerical columns\n",
        "\n",
        "df = fi_selectkbest(numerical_df, 'likes', 10) # feature importance\n",
        "#df = fi_select_linear(numerical_df, 'likes')\n",
        "\n",
        "#Modeling pipeline\n",
        "#model = fit_crossvalidate_mlr(df, 10, 'likes')\n",
        "model = fit_crossvalidate_reg(df, 'likes', 5, 2)\n",
        "#clustering_model(df)\n",
        "\n",
        "#print(df.shape)\n",
        "#print(df.dtypes)\n",
        "#df.head()\n",
        "\n",
        "# Deployment pipeline\n",
        "dump_pickle(model, 'Group_Project_Regression_Model.sav')\n",
        "\n",
        "#Saved model\n",
        "model = load_pickle('Group_Project_Regression_Model.sav')\n",
        "model.predict(df.drop(columns=['likes']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hoRVtcEjPU-",
        "outputId": "2413f063-3b05-4924-cb10-a86fc9e559b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "          R-squared\n",
            "Lasso     -0.412137\n",
            "Ridge     -0.441895\n",
            "MLR       -0.463956\n",
            "AdaBoost  -1.140375\n",
            "XGBoost   -1.868410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.71575065e+00, -1.66834272e+00, -3.85468656e+00,  9.90902801e+00,\n",
              "       -1.52207412e+00, -5.04184336e+00,  4.09056288e+00,  5.45653553e-02,\n",
              "        1.06682346e+00, -8.31235166e-01, -3.42069968e+00,  3.78567319e-01,\n",
              "       -3.39210120e+00,  4.09056288e+00,  5.79669507e-02,  4.14803963e+01,\n",
              "       -2.02434476e+00, -4.51913153e+00, -1.52207412e+00,  5.79669507e-02,\n",
              "        1.83211826e+01,  4.09056288e+00, -1.52207412e+00,  9.70319987e+00,\n",
              "       -3.42069968e+00,  5.75139184e+00, -3.92498620e+00,  4.68432679e+01,\n",
              "       -1.52207412e+00, -1.52207412e+00,  1.04833307e+01, -1.52207412e+00,\n",
              "       -3.74728446e+00, -2.54595434e+00, -2.65080525e+00,  2.78473597e+01,\n",
              "       -3.42069968e+00,  1.53158369e+01, -3.81123531e-01,  3.16488202e-01,\n",
              "       -3.81123531e-01, -1.52207412e+00, -3.81123531e-01, -3.81123531e-01,\n",
              "       -1.52207412e+00, -1.09003988e+00,  1.16330383e+01,  7.80457432e+00,\n",
              "       -4.77198346e+00, -3.78353652e+00,  2.88627359e+01,  9.81500107e-01,\n",
              "       -3.39210120e+00, -2.28144868e+00, -3.42069968e+00,  1.90298483e+01,\n",
              "        2.35032343e+01,  2.46710838e+01,  1.93021595e+02,  1.51668901e+00,\n",
              "       -1.52207412e+00,  1.97806448e+01, -3.81123531e-01, -3.81123531e-01,\n",
              "       -1.52207412e+00, -3.81123531e-01, -3.81123531e-01,  5.25563183e+01,\n",
              "       -1.52207412e+00, -2.73688603e-01, -1.52207412e+00, -3.42069968e+00,\n",
              "       -6.83013918e-01, -1.52207412e+00,  9.70319987e+00, -1.52207412e+00,\n",
              "       -1.17678057e+00, -1.52207412e+00, -1.52207412e+00, -3.42069968e+00,\n",
              "       -1.18103256e+00, -1.52207412e+00, -1.52207412e+00, -1.52207412e+00,\n",
              "       -1.52207412e+00,  9.70319987e+00, -1.52207412e+00, -4.98193900e-01,\n",
              "        4.09056288e+00,  1.28994760e+00, -1.52207412e+00, -2.23073983e+00,\n",
              "        7.35451371e-01, -2.23073983e+00,  4.09056288e+00,  1.17312332e+00,\n",
              "        4.09056288e+00, -2.16355832e+00, -3.20186371e+00, -2.02636064e+00,\n",
              "       -5.10845794e+00,  3.47952960e-01,  1.63941426e+01, -3.42069968e+00,\n",
              "        2.64419293e+01, -2.23073983e+00, -1.52207412e+00, -1.52207412e+00,\n",
              "        1.53158369e+01,  1.17312332e+00, -2.23073983e+00, -2.23073983e+00,\n",
              "       -2.23073983e+00, -2.23073983e+00, -2.23073983e+00,  6.19529883e+00,\n",
              "       -1.52207412e+00, -2.27974909e+00, -2.33842540e+00, -1.72160277e+00,\n",
              "       -1.52207412e+00,  4.36110975e-01, -1.52207412e+00,  3.30557063e+00,\n",
              "       -1.28052923e+00, -2.30302374e+00, -4.09988490e+00,  2.99213102e+00,\n",
              "       -1.52207412e+00, -1.68475215e+00,  2.83322647e-01, -1.52207412e+00,\n",
              "       -1.52207412e+00, -1.52207412e+00,  1.52226081e+01, -1.30323815e+00,\n",
              "       -1.52207412e+00,  4.09056288e+00, -1.29300174e+00, -3.74728446e+00,\n",
              "        1.53158369e+01, -1.44893982e+00, -1.52207412e+00, -1.52207412e+00,\n",
              "       -4.44457990e+00, -3.42069968e+00, -1.52207412e+00, -1.52207412e+00,\n",
              "       -1.52207412e+00, -1.52207412e+00, -1.52207412e+00, -2.27974909e+00,\n",
              "       -1.52207412e+00, -1.52207412e+00,  2.19193732e+00,  1.15918616e+01,\n",
              "       -2.12015636e+00,  1.23187795e+01,  4.14803963e+01,  1.44410915e+01,\n",
              "       -9.67298982e-01, -1.29470254e+00,  7.80457432e+00, -3.42069968e+00,\n",
              "       -1.52207412e+00, -3.85979016e+00,  4.60611395e+00, -1.52207412e+00,\n",
              "       -3.14743818e+00, -1.84065861e+00, -1.58213735e+00,  2.19193732e+00,\n",
              "        4.07110688e+01, -4.09594799e+00,  1.23288789e-02,  4.09056288e+00,\n",
              "       -4.22744472e+00, -4.38448505e+00, -2.27974909e+00, -1.52207412e+00,\n",
              "       -1.52207412e+00, -5.96906158e+00,  1.04333037e+02, -5.31652219e+00,\n",
              "       -3.42069968e+00, -3.81123531e-01,  4.69436813e+01, -2.62050597e+00,\n",
              "       -5.20020804e-01,  4.99371687e+00, -1.22034140e-01,  4.09056288e+00,\n",
              "       -1.22034140e-01,  2.73080673e+00, -2.02636064e+00,  7.81977578e+00,\n",
              "       -3.42069968e+00, -2.62050597e+00, -1.44893982e+00, -4.82073966e+00,\n",
              "       -2.91641316e+00, -2.27974909e+00, -3.42069968e+00, -1.52207412e+00,\n",
              "        1.21175184e+01, -1.52207412e+00, -1.15438673e+00, -1.52207412e+00,\n",
              "       -1.52207412e+00, -8.30951699e-01, -1.52207412e+00,  3.21537479e+01,\n",
              "       -1.67284657e+00, -3.42069968e+00, -6.02887296e+00, -4.06218388e+00,\n",
              "       -8.13408409e-01, -1.52207412e+00,  4.07764346e-01, -5.28902596e+00,\n",
              "       -3.74728446e+00,  8.91820763e+00,  3.30557063e+00,  6.04206482e-01,\n",
              "       -1.52207412e+00, -1.52207412e+00,  4.09056288e+00, -1.52207412e+00,\n",
              "        9.25195317e+00, -1.52207412e+00, -1.52207412e+00, -1.52207412e+00,\n",
              "       -3.82823122e-01, -3.82823122e-01, -1.52207412e+00, -1.52207412e+00,\n",
              "        1.08424509e+01, -4.98193900e-01, -1.69637427e+00, -3.42069968e+00,\n",
              "        9.50055310e+00, -8.21597312e-01,  3.30557063e+00, -1.79955599e+00,\n",
              "       -8.13408409e-01,  4.09056288e+00,  8.91820763e+00, -1.52207412e+00,\n",
              "       -3.42069968e+00, -1.52207412e+00, -1.15923728e+00, -1.52207412e+00,\n",
              "       -9.48621827e-01,  5.67060395e+00, -1.84065861e+00,  4.09056288e+00,\n",
              "        5.79669507e-02,  5.79669507e-02,  1.05179975e+00,  3.30557063e+00,\n",
              "       -1.52207412e+00,  1.32320762e+01, -2.27974909e+00, -1.52207412e+00,\n",
              "        1.75892365e+01, -3.77871760e+00,  1.02766522e+01,  1.13807895e+01,\n",
              "       -1.52207412e+00, -3.42069968e+00,  7.09590860e+00, -1.52207412e+00,\n",
              "       -1.58213735e+00, -1.52207412e+00,  3.76016122e-01, -3.42069968e+00,\n",
              "       -1.30323815e+00,  1.56131189e+01,  5.83183073e+01,  9.70319987e+00,\n",
              "        1.51668901e+00, -3.56696828e+00, -2.43707167e+00, -3.83969447e+00,\n",
              "        2.65411109e+01,  1.41750743e+00, -5.67822517e+00,  1.37603891e+02,\n",
              "       -2.27974909e+00, -2.98671280e+00, -3.42069968e+00,  1.55515529e-01,\n",
              "        2.36825335e+00, -1.52207412e+00,  5.83183073e+01, -3.74728446e+00,\n",
              "        9.21972187e-01, -8.59014861e-01, -4.55995068e+00, -5.62691777e+00,\n",
              "        9.73441278e+00,  1.90298483e+01,  2.13185551e+01,  4.43132097e+00,\n",
              "        1.02766522e+01, -3.77959961e+00, -2.39681946e+00, -1.88346201e+00,\n",
              "       -7.22951045e-01, -1.52207412e+00, -2.33842540e+00,  4.36045440e+00,\n",
              "        1.53158369e+01, -1.52207412e+00, -3.82823122e-01, -1.15923728e+00,\n",
              "        4.88268771e-01, -1.52207412e+00, -1.52207412e+00, -1.52207412e+00,\n",
              "        3.81510802e+01, -8.67802316e-01,  5.04390377e+01, -2.27974909e+00,\n",
              "       -3.42069968e+00,  4.09056288e+00,  4.50473874e+00,  2.95131188e+00,\n",
              "       -2.84724738e+00, -1.52207412e+00, -3.42069968e+00,  4.09056288e+00,\n",
              "       -1.52207412e+00,  6.02169328e+01, -1.52207412e+00, -2.73688603e-01,\n",
              "        4.09056288e+00, -6.83013918e-01, -1.49312894e+00,  5.43326857e+00,\n",
              "        8.92775160e-01, -2.28541721e+00, -2.28144868e+00, -3.42069968e+00,\n",
              "       -3.83969447e+00, -1.44893982e+00, -3.42069968e+00, -2.27974909e+00,\n",
              "       -3.42069968e+00, -7.46952706e-02, -2.45691431e+00,  9.78968871e+00,\n",
              "       -2.27974909e+00, -3.36063644e+00, -2.66132512e+00,  4.83860225e+02,\n",
              "        2.62025488e+00, -8.80589918e-01, -3.60583478e+00,  1.16330383e+01,\n",
              "        1.74843539e+01, -1.52207412e+00, -2.27974909e+00, -2.84724738e+00,\n",
              "       -2.33842540e+00, -4.09594799e+00, -3.42069968e+00,  1.17835589e+01,\n",
              "        5.49404063e+00,  3.30557063e+00, -1.37917549e+00,  1.50780403e+01,\n",
              "       -2.27974909e+00, -1.52207412e+00,  8.91820763e+00,  1.04112246e+01,\n",
              "        5.86194369e+00,  3.81308100e+00, -1.52207412e+00, -3.74728446e+00,\n",
              "        1.56786737e+01, -1.52207412e+00, -5.11979659e+00, -2.22588929e+00,\n",
              "       -1.52207412e+00,  5.26375573e+00, -3.42069968e+00, -1.32960051e+00,\n",
              "       -3.42069968e+00,  1.00591582e+01,  5.61686539e-01,  1.27727478e+03,\n",
              "        2.02025875e+02, -2.73688603e-01, -2.73688603e-01, -2.73688603e-01,\n",
              "       -2.73688603e-01, -2.73688603e-01, -1.52207412e+00, -3.42069968e+00,\n",
              "       -1.52207412e+00, -2.73688603e-01, -2.73688603e-01,  4.09056288e+00,\n",
              "        2.59676586e+01, -4.77198346e+00, -2.73688603e-01, -2.73688603e-01,\n",
              "       -2.73688603e-01, -2.73688603e-01,  9.34114607e+01, -1.52207412e+00,\n",
              "       -3.32747089e+00, -3.56696828e+00, -1.52207412e+00, -3.32747089e+00,\n",
              "        1.35634799e+01, -3.32747089e+00,  3.30557063e+00, -1.52207412e+00,\n",
              "       -1.52207412e+00,  7.95084292e+00, -2.73688603e-01, -2.73688603e-01,\n",
              "       -2.73688603e-01, -2.73688603e-01, -2.73688603e-01, -2.73688603e-01,\n",
              "       -2.73688603e-01, -2.73688603e-01, -2.73688603e-01, -2.73688603e-01,\n",
              "       -2.73688603e-01, -1.74091009e+00,  5.67545905e+00, -1.52207412e+00,\n",
              "       -3.02157915e+00, -1.15923728e+00,  2.50053221e+01, -2.73688603e-01,\n",
              "       -3.81123531e-01, -4.32268940e+00, -2.73688603e-01,  8.35231504e-01,\n",
              "        3.14499327e+01])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Model Pipeline \n",
        "# LABEL = source\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data retrival pipeline\n",
        "#data = get_data('self driving')\n",
        "df = data\n",
        "\n",
        "# Data cleaning pipeline\n",
        "df = bin_source_label_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "# df = impute_KNN(df)    #not currently applicable since we don't have missing data\n",
        "df = remove_retweets(df)\n",
        "df = get_sentiment(df)\n",
        "df = dummy_codes(df.drop(columns=['source'])).join(df['source'])\n",
        "\n",
        "df[['retweets', 'likes']] = df[['retweets', 'likes']].apply(pd.to_numeric) # converting \"retweets\" and \"likes\" datatype from an object to numeric\n",
        "df = df.drop(columns=['url', 'text']) # droping string comlumns\n",
        "\n",
        "#df = image_cleaning(df)\n",
        "#numerical_df = df.select_dtypes(include=np.number) # including only numerical columns\n",
        "\n",
        "df = fs_variance(df, 'source', p=.8) # feature importance\n",
        "#df = fi_selectkbest(numerical_df, 'likes', 10) # feature importance\n",
        "#df = fi_select_linear(numerical_df, 'likes')\n",
        "\n",
        "#Modeling pipeline\n",
        "model = fit_crossvalidate_clf(df, 'source', 5, 2)\n",
        "\n",
        "#print(df.shape)\n",
        "#print(df.dtypes)\n",
        "df.head()\n",
        "\n",
        "# Deployment pipeline\n",
        "dump_pickle(model, 'Group_Project_Classification_Model.sav')\n",
        "\n",
        "#Saved model\n",
        "model = load_pickle('Group_Project_Classification_Model.sav')\n",
        "model.predict(df.drop(columns=['source']))"
      ],
      "metadata": {
        "id": "vjbapOuaMYFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5735b48a-0454-420a-9e06-034c9d379af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "            Accuracy\n",
            "XGBoost     0.470787\n",
            "ExtraTrees  0.444944\n",
            "Logistic    0.384270\n",
            "AdaBoost    0.378652\n",
            "KNN         0.350562\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'HubSpot', 'Echobox',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'dlvr.it', 'tweeter_biases', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'HubSpot', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'Direct to Twitter', 'The Social Jukebox', 'TweetDeck',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'IFTTT', 'Echobox', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Echobox',\n",
              "       'Direct to Twitter', 'eClincher', 'dlvr.it', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Zift Platform', 'dlvr.it',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'Direct to Twitter', 'dlvr.it', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter', 'dlvr.it',\n",
              "       'dlvr.it', 'Travelers ToolkitPlus', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'HubSpot', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'Travelers ToolkitPlus', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter', 'dlvr.it',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'tweeter_biases', 'Direct to Twitter', 'Sendible', 'dlvr.it',\n",
              "       'dlvr.it', 'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'eClincher', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'True Anthem',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'TweetDeck', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'dlvr.it', 'Sendible', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it', 'eClincher',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'tweeter_biases', 'Hootsuite Inc.',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'dlvr.it', 'Direct to Twitter', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'dlvr.it', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'HubSpot',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'HubSpot', 'dlvr.it', 'HubSpot',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'eClincher',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'TweetDeck',\n",
              "       'SocialFlow', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'tweeter_biases', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'TweetDeck', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Travelers ToolkitPlus', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'SocialFlow', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it', 'dlvr.it',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Sprout Social', 'dlvr.it', 'dlvr.it', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'TweetDeck', 'Sendible',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'The Social Jukebox', 'Direct to Twitter',\n",
              "       'dlvr.it', 'dlvr.it', 'Direct to Twitter', 'dlvr.it', 'eClincher',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'Hootsuite Inc.',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it',\n",
              "       'tweeter_biases', 'HubSpot', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'dlvr.it', 'Direct to Twitter', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'TweetDeck', 'Zift Platform', 'Zift Platform', 'Zift Platform',\n",
              "       'Zift Platform', 'Zift Platform', 'dlvr.it', 'Direct to Twitter',\n",
              "       'dlvr.it', 'Zift Platform', 'Zift Platform', 'dlvr.it',\n",
              "       'TweetDeck', 'Direct to Twitter', 'Zift Platform', 'Zift Platform',\n",
              "       'Zift Platform', 'Zift Platform', 'Direct to Twitter', 'dlvr.it',\n",
              "       'dlvr.it', 'Direct to Twitter', 'dlvr.it', 'dlvr.it',\n",
              "       'Direct to Twitter', 'dlvr.it', 'dlvr.it', 'dlvr.it', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Zift Platform', 'Zift Platform',\n",
              "       'Zift Platform', 'Zift Platform', 'Zift Platform', 'Zift Platform',\n",
              "       'Zift Platform', 'Zift Platform', 'Zift Platform', 'Zift Platform',\n",
              "       'Zift Platform', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'dlvr.it', 'Direct to Twitter', 'Direct to Twitter',\n",
              "       'Direct to Twitter', 'Sprout Social', 'dlvr.it',\n",
              "       'Direct to Twitter', 'Zift Platform', 'Direct to Twitter',\n",
              "       'Direct to Twitter'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering Model Pipeline \n",
        "# LABEL = source    (I don't know what the label should be for the clustering model)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data retrival pipeline\n",
        "#data = get_data('self driving')\n",
        "\n",
        "# data = data.astype(str)\n",
        "# data.dtypes.head()\n",
        "df = data\n",
        "\n",
        "# Data cleaning pipeline\n",
        "df = bin_source_label_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "# df = impute_KNN(df)    #not currently applicable since we don't have missing data\n",
        "df = remove_retweets(df)\n",
        "df = get_sentiment(df)\n",
        "df = dummy_codes(df.drop(columns=['source'])).join(df['source'])\n",
        "\n",
        "#df[['retweets', 'likes']] = df[['retweets', 'likes']].apply(pd.to_numeric) # converting \"retweets\" and \"likes\" datatype from an object to numeric\n",
        "df = df.drop(columns=['url', 'text']) # droping string comlumns\n",
        "\n",
        "#df = image_cleaning(df)\n",
        "#numerical_df = df.select_dtypes(include=np.number) # including only numerical columns\n",
        "\n",
        "df = fs_variance(df, 'source', p=.8) # feature importance\n",
        "#df = fi_selectkbest(numerical_df, 'likes', 10) # feature importance\n",
        "#df = fi_select_linear(numerical_df, 'likes')\n",
        "\n",
        "\n",
        "#Modeling pipeline\n",
        "model = clustering_model(df, 'source')\n",
        "\n",
        "#print(df.shape)\n",
        "print(df.dtypes)\n",
        "#df.head()\n",
        "\n",
        "# Deployment pipeline\n",
        "dump_pickle(model, 'Group_Project_Clustering_Model.sav')\n",
        "\n",
        "#Saved model\n",
        "model = load_pickle('Group_Project_Clustering_Model.sav')\n",
        "#model.predict(df.drop(columns=['source']))"
      ],
      "metadata": {
        "id": "I-eMYdy-pI18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f923e8-e42d-4b7f-8d4b-5b39667f6a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Collecting gower\n",
            "  Downloading gower-0.0.5.tar.gz (4.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gower) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gower) (1.4.1)\n",
            "Building wheels for collected packages: gower\n",
            "  Building wheel for gower (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gower: filename=gower-0.0.5-py3-none-any.whl size=4231 sha256=85dd73f7992b757e66f485aef98fb318559afd9edb07d02542f2018996b08ea0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/f9/9a/67122a959a424e9cbb4557a8366c871a30e31cd75f0d003db4\n",
            "Successfully built gower\n",
            "Installing collected packages: gower\n",
            "Successfully installed gower-0.0.5\n",
            "0    443\n",
            "1      2\n",
            "Name: agg_cluster, dtype: int64 \n",
            "\n",
            "\n",
            "retweets              object\n",
            "likes                 object\n",
            "sentiment_overall    float64\n",
            "source                object\n",
            "agg_cluster            int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "85tFzOEH4PyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}